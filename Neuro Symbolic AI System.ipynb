{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aef042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  category  \\\n",
      "0  in the end we all die, nothing we do means any...       5.0   \n",
      "1  Today would have been my best friend's 18th bi...       4.0   \n",
      "2  So I couldn't feel any worse, and I've had eno...       5.0   \n",
      "3  I am 22 and have never had a girlfriend and ha...       4.0   \n",
      "4  I am almost certain that my depression is caus...       4.0   \n",
      "\n",
      "                                         explanation  \n",
      "0  feel more and more empty,fear lonliness,get ti...  \n",
      "1   best friend's birthday, he's gone, never supp...  \n",
      "2  absolute minimal contact with,  be more direct...  \n",
      "3   never had a girlfriend, rejected countless times  \n",
      "4  eating out with family, what after that? Paren...  \n",
      "Index(['text', 'category', 'explanation'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\My Office Work\\IXD Labs\\Workspace\\Neuro Symbolic AI\\\\dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and the columns\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d7efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in the end we all die, nothing we do means any...</td>\n",
       "      <td>end die nothing means anything anything throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today would have been my best friend's 18th bi...</td>\n",
       "      <td>today would best friends th birthday wed going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
       "      <td>couldnt feel worse ive enough tonight midnight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
       "      <td>never girlfriend rejected countless times see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am almost certain that my depression is caus...</td>\n",
       "      <td>almost certain depression caused bullshit shap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  in the end we all die, nothing we do means any...   \n",
       "1  Today would have been my best friend's 18th bi...   \n",
       "2  So I couldn't feel any worse, and I've had eno...   \n",
       "3  I am 22 and have never had a girlfriend and ha...   \n",
       "4  I am almost certain that my depression is caus...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  end die nothing means anything anything throug...  \n",
       "1  today would best friends th birthday wed going...  \n",
       "2  couldnt feel worse ive enough tonight midnight...  \n",
       "3  never girlfriend rejected countless times see ...  \n",
       "4  almost certain depression caused bullshit shap...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text cleaning\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters, digits, and punctuation\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x) if isinstance(x, str) else '')\n",
    "\n",
    "df[['text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672d5dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     5052.000000\n",
      "mean       567.680325\n",
      "std        775.536087\n",
      "min          0.000000\n",
      "25%        157.000000\n",
      "50%        339.500000\n",
      "75%        688.250000\n",
      "max      12615.000000\n",
      "Name: cleaned_text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# EDA (Exploratory Data Analysiz)\n",
    "\n",
    "df['cleaned_text_length'] = df['cleaned_text'].apply(len)\n",
    "print(df['cleaned_text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b5708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33ec10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values in features (X):\n",
      "0\n",
      "Checking for NaN values in target (y):\n",
      "9\n",
      "Accuracy: 0.45391476709613476\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Vectorize the cleaned text data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])  # Transform the text data into feature vectors\n",
    "y = df['category']  # Adjust to your actual target column\n",
    "\n",
    "# Convert sparse matrix to DataFrame for easier NaN handling\n",
    "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Check for NaN values in features (X)\n",
    "print(\"Checking for NaN values in features (X):\")\n",
    "print(X_df.isnull().sum().sum())  # Check for NaN values in features\n",
    "\n",
    "# Check for NaN values in target (y)\n",
    "print(\"Checking for NaN values in target (y):\")\n",
    "print(pd.isnull(y).sum())  # Check for NaN values in target\n",
    "\n",
    "# Remove rows with NaN in the target variable\n",
    "df = df[~pd.isnull(df['category'])]\n",
    "\n",
    "# Re-apply vectorization after removing NaN rows\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])  # Transform the cleaned text again\n",
    "y = df['category']  # Update target variable after dropping NaN rows\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features (if any)\n",
    "imputer = SimpleImputer(strategy='mean')  # Using 'mean' for numerical data; adjust accordingly\n",
    "X_train_imputed = imputer.fit_transform(X_train.toarray())  # .toarray() converts sparse to dense\n",
    "X_test_imputed = imputer.transform(X_test.toarray())\n",
    "\n",
    "# Train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_imputed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11ec456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.14      0.25       147\n",
      "         1.0       0.00      0.00      0.00        63\n",
      "         2.0       0.64      0.31      0.42       119\n",
      "         3.0       0.57      0.24      0.34       124\n",
      "         4.0       0.44      0.75      0.56       283\n",
      "         5.0       0.41      0.58      0.48       273\n",
      "\n",
      "    accuracy                           0.45      1009\n",
      "   macro avg       0.49      0.34      0.34      1009\n",
      "weighted avg       0.51      0.45      0.41      1009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the data model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d080c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3405176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  depression_flag\n",
      "0  end die nothing means anything anything throug...                1\n",
      "1  today would best friends th birthday wed going...                0\n",
      "2  couldnt feel worse ive enough tonight midnight...                1\n",
      "3  never girlfriend rejected countless times see ...                1\n",
      "4  almost certain depression caused bullshit shap...                0\n"
     ]
    }
   ],
   "source": [
    "# Define symbolic depression keywords\n",
    "depressive_keywords = [\n",
    "    \"hopeless\", \"tired\", \"exhausted\", \"sad\", \"depressed\", \"alone\", \"worthless\",\n",
    "    \"guilty\", \"failure\", \"empty\", \"helpless\", \"useless\"\n",
    "]\n",
    "\n",
    "# Function to flag depressive content based on keywords\n",
    "def flag_depression(text):\n",
    "    return int(any(word in text for word in depressive_keywords))\n",
    "\n",
    "# Apply symbolic flagging\n",
    "df['depression_flag'] = df['cleaned_text'].apply(flag_depression)\n",
    "\n",
    "print(df[['cleaned_text', 'depression_flag']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13933f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the cleaned text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_text_features = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# Add the depression_flag as an additional feature\n",
    "import scipy.sparse\n",
    "\n",
    "X_combined = scipy.sparse.hstack((X_text_features, df[['depression_flag']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc24c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45391476709613476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the target variable (e.g., category for depressive messages)\n",
    "y = df['category']  # Replace 'category' with the actual column name for labels\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes model on the combined features\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
